___Read and write csv file :-

df = spark.read.format('csv').option('header',True).schema(Schema).load('path')
	                     .option('delimiter','|').option('inferSchema',True)
			     .option('dateFormat','yyyy-MM-dd')
			     .option('mode','PERMISSIVE')   --- 'DROPMALFORMED' , 'FAILFASTs'

df.write.mode('overwrite').format('csv').option('header',True).option('delimiter',',').save('path')

# mode('append') -- appends to existing file
-------------------------------------------------------
___Read and write excel file :-
# First read excel file into pandas dataframe and then convert pandas df into pyspark
import pandas as pd
df = pd.read_excel('file_path', sheet_name='sheet1')
df1 = spark.createDataFrame(df)
--- OR ----
import pyspark.pandas as ps
spark_df = ps.read_excel('<excel file path>', sheet_name='Sheet1', inferSchema='').to_spark()
-------------------------------------------------------
___Read and write parquet file :-

df = spark.read.format('parquet').load('path')

df.write.mode('overwrite').partitionBy('gender','salary').parquet('path')
-------------------------------------------------------
___Read and write from jdbc

host='jdbc:mysql://pranavsql.cjamoxxbcrxr.ap-south-1.rds.amazonaws.com:3306/SYSTEM'

df=spark.read.format('jdbc').option('url',host).option('user',myuser).option('password',mypassword).option('dbtable','health')
.option('driver','com.mysql.jdbc.Driver').load()

df.write.format('jdbc').option('url','asknl').option('user',user).option('password',pass).option('dbtable',table).option('driver','scc').save()
-------------------------------------------------------

___Write data to Snowflake

res.write.mode('overwrite').format('net.snowflake.spark.snowflake').option('sfUrl',snowflake http url).option('sfUser',username)
.option('sfPassword',password).option('sfDatabase',db name).option('sfSchema',schema name).option('sfWarehouse',cluster name)
.option('dbtable','healthres').save()
-------------------------------------------------------

___Read and write Hive Table
spark = SparkSession \
    .builder \
    .appName("SparkByExamples.com") \
    .enableHiveSupport() \
    .getOrCreate()

df = spark.sql('select * from db.dep')
OR
df = spark.read.table('dep')

# Save to Hive Table as Internal table

df.write.mode('overwrite').saveAsTable('database.tablename')

# Save to Hive Table as External table

df.write.mode(Savemode.Overwrite).option('path','hdfs path').saveAsTable('database.tablename')

-------------------------------------------------------
