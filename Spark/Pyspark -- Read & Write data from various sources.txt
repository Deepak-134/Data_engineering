PySpark Saving modes:-
PySpark DataFrameWriter also has a method mode() to specify SaveMode; the argument to this method either takes overwrite, append, ignore, errorifexists.

overwrite – mode is used to overwrite the existing file

append – To add the data to the existing file

ignore – Ignores write operation when the file already exists

errorifexists or error – This is a default option when the file already exists, it returns an error

-------------------------------------------------------
___Read and write csv file :-

df = spark.read.format('csv').option('header',True).schema(Schema).load('path')
	                     .option('delimiter','|').option('inferSchema',True)
			     .option('dateFormat','yyyy-MM-dd')
			     .option('mode','PERMISSIVE')   --- 'DROPMALFORMED' , 'FAILFASTs'

df.write.mode('overwrite').format('csv').option('header',True).option('delimiter',',').save('path')

# mode('append') -- appends to existing file
-------------------------------------------------------
___Read and write excel file :-
# First read excel file into pandas dataframe and then convert pandas df into pyspark
import pandas as pd
df = pd.read_excel('file_path', sheet_name='sheet1')
df1 = spark.createDataFrame(df)
--- OR ----
import pyspark.pandas as ps
spark_df = ps.read_excel('<excel file path>', sheet_name='Sheet1', inferSchema='').to_spark()
-------------------------------------------------------
___Read and write parquet file :-

df = spark.read.format('parquet').load('path')

df.write.mode('overwrite').partitionBy('gender','salary').parquet('path')
-------------------------------------------------------
___Read and write from jdbc

host='jdbc:mysql://pranavsql.cjamoxxbcrxr.ap-south-1.rds.amazonaws.com:3306/SYSTEM'

df=spark.read.format('jdbc').option('url',host).option('user',myuser).option('password',mypassword).option('dbtable','health')
.option('driver','com.mysql.jdbc.Driver').load()

df.write.format('jdbc').option('url','asknl').option('user',user).option('password',pass).option('dbtable',table).option('driver','scc').save()
-------------------------------------------------------

___Write data to Snowflake

res.write.mode('overwrite').format('net.snowflake.spark.snowflake').option('sfUrl',snowflake http url).option('sfUser',username)
.option('sfPassword',password).option('sfDatabase',db name).option('sfSchema',schema name).option('sfWarehouse',cluster name)
.option('dbtable','healthres').save()
-------------------------------------------------------

___Read and write Hive Table
spark = SparkSession \
    .builder \
    .appName("SparkByExamples.com") \
    .enableHiveSupport() \
    .getOrCreate()

df = spark.sql('select * from db.dep')
OR
df = spark.read.table('dep')

# Save to Hive Table as Internal table

df.write.mode('overwrite').saveAsTable('database.tablename')

# Save to Hive Table as External table

df.write.mode(Savemode.Overwrite).option('path','hdfs path').saveAsTable('database.tablename')

-------------------------------------------------------

___Read and write data in XML format

XML is an extensible markup language that is used to represent structured data in a hierarchical format.
PySpark provides support for reading and writing XML files using the spark-xml package, which is an external package developed by Databricks.
Ex:- emp.xml

<root>
  <row>
    <id>1</id>
    <name>John</name>
  </row>
  <row>
    <id>2</id>
    <name>Jane</name>
  </row>
  <row>
    <id>3</id>
    <name>Jim</name>
  </row>
</root>

# Read XML data

df = spark.read \
    .format('com.databricks.spark.xml') \
    .option('rowTag','row') \                    # The rowTag option specifies the tag name that represents each row in the XML file.
    .option('treatEmptyValuesAsNulls',True)      # Treats empty values as NULL
    .load('emp.xml')

# Write xml data

df.select(struct("id", "name").alias("root")) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('rootTag', 'root') \                  #  The rootTag option specifies the tag name for the root element,
    .option('rowTag', 'row') \
    .save(xmlFile)
    
-------------------------------------------------------
___Read and write data in JSON (Javascript Object Notation) format 

# Read JSON data
df = spark.read.format('json')
		.schema(sch)                      # By default JSON reader inferSchema automatically , if you want to provide schema, use this option 
		.option("multiline","true")       # To read json data with multiline row format (1 record consist of multiple lines in json file)
		.option('dateFormat','YYYY-MMM-dd')     # Option to specify date format of source data
		.load('emp.json')

# Write JSON data
df.write.format('json').save('path')
-------------------------------------------------------
